{"url": ["/url?q=http://scrapy.org/&sa=U&ved=0ahUKEwj7nsqk5N_OAhUHDSwKHXEyAu0QFggTMAA&usg=AFQjCNFbbx5XIu2vDoqKJtBGZLAW1otLvQ"], "text": ["PyPI Version PyPI monthly downloads Wheel Status Python 3 Porting Status ", "\nCoverage report. Install the latest version of ", ". ", " 1.1. pip install ", "."]}
{"url": ["/url?q=http://scrapy.org/doc/&sa=U&ved=0ahUKEwj7nsqk5N_OAhUHDSwKHXEyAu0QFggeMAE&usg=AFQjCNGclhFlzk11HhVjH6uuhDkLstXVTw"], "text": ["Documentation. You can also find a lot of useful information at ", " Wiki or ", "\nlooking for the ", " Snipplr tag. PyPI Version PyPI monthly downloads Wheel\u00a0..."]}
{"url": ["/url?q=http://doc.scrapy.org/en/latest/intro/tutorial.html&sa=U&ved=0ahUKEwj7nsqk5N_OAhUHDSwKHXEyAu0QFggkMAI&usg=AFQjCNH6Vh_g9mi9wZJX3VWBndnKHosb1A"], "text": [" is written in Python. If you're new to the language you might want to start ", "\nby getting an idea of what the language is like, to get the most out of ", "."]}
{"url": ["/url?q=https://habrahabr.ru/post/115710/&sa=U&ved=0ahUKEwj7nsqk5N_OAhUHDSwKHXEyAu0QFggqMAM&usg=AFQjCNGtf0QhoPGOqfhYcKYUGtRqJD63Xw"], "text": ["18 \u043c\u0430\u0440 2011 ", " \u0422\u0430\u043c \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u043f\u0438\u0441\u0430\u0442\u044c \u0441\u0432\u043e\u0438 \u043a\u043b\u0430\u0441\u0441\u044b \u0434\u043b\u044f \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0430\u0445, \u043d\u0435 ", "\n\u043f\u0440\u0435\u0434\u0443\u0441\u043c\u043e\u0442\u0440\u0435\u043d\u043d\u044b\u0445 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u043e\u043c ", "."]}
{"url": ["/url?q=https://github.com/scrapy/scrapy&sa=U&ved=0ahUKEwj7nsqk5N_OAhUHDSwKHXEyAu0QFggxMAQ&usg=AFQjCNHYUOWYbRkgyGuXUgyIQCA-5ZnkqQ"], "text": ["README.rst. ", ". PyPI Version Build Status Wheel Status Python 3 Porting ", "\nStatus Coverage report. Overview. ", " is a fast high-level web crawling and\u00a0..."]}
{"url": ["/url?q=https://en.wikipedia.org/wiki/Scrapy&sa=U&ved=0ahUKEwj7nsqk5N_OAhUHDSwKHXEyAu0QFgg3MAU&usg=AFQjCNHGLcj4ZGCbv1MyLy1rLaj4TMUmmg"], "text": [" is a free and open source web crawling framework, written in Python. ", "\nOriginally designed for web scraping, it can also be used to extract data using ", "\nAPIs\u00a0..."]}
{"url": [], "text": []}
{"url": ["/url?q=https://scrapinghub.com/scrapy-cloud/&sa=U&ved=0ahUKEwj7nsqk5N_OAhUHDSwKHXEyAu0QFghBMAc&usg=AFQjCNHIo-gMz5ZQI7qqdIilIWRaA23mUA"], "text": [" Cloud is a robust, fully-featured production environment to deploy and run ", "\nyour crawls - think of it as a Heroku for your web crawlers. It runs on top of the\u00a0..."]}
{"url": ["/url?q=https://realpython.com/blog/python/web-scraping-with-scrapy-and-mongodb&sa=U&ved=0ahUKEwj7nsqk5N_OAhUHDSwKHXEyAu0QFghGMAg&usg=AFQjCNEdhDHua9wJl9eL95bqq51gkghPLA"], "text": ["31 Dec 2014 ", " This tutorial covers how to write a crawler using ", " to scrape and parse data ", "\nand then store the data in MongoDB."]}
{"url": ["/url?q=http://gis-lab.info/qa/scrapy.html&sa=U&ved=0ahUKEwj7nsqk5N_OAhUHDSwKHXEyAu0QFghNMAk&usg=AFQjCNFuEA__UhOAvflKMqykz-ayhBH44A"], "text": ["2 \u044f\u043d\u0432 2013 ", " \u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0442\u044c\u0441\u044f \u043f\u0430\u0440\u0441\u0438\u043d\u0433, \u0432\u044b\u0431\u0440\u0430\u043d ", "\n", " - \u043e\u0447\u0435\u043d\u044c \u0433\u0438\u0431\u043a\u0438\u0439 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a, \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043d\u044b\u0439 \u043d\u0430 Python \u0438\u00a0..."]}
{"url": ["/url?q=https://media.readthedocs.org/pdf/scrapy/1.1/scrapy.pdf&sa=U&ved=0ahUKEwiF3Oak5N_OAhUGCywKHcrzDSA4ChAWCBMwAA&usg=AFQjCNEUog5Pcnfw5_etgQMbZOLKU2D2eA"], "text": ["18 Aug 2016 ", " ", " Documentation, Release 1.1.2. This documentation contains everything ", "\nyou need to know about ", ". First steps. 1\u00a0..."]}
{"url": ["/url?q=https://pypi.python.org/pypi/Scrapy/1.1.1&sa=U&ved=0ahUKEwiF3Oak5N_OAhUGCywKHcrzDSA4ChAWCBYwAQ&usg=AFQjCNHXvmafMAs64-TLKQNFwG_jIHfEXw"], "text": ["13 Jul 2016 ", " ", " is a fast high-level web crawling and web scraping framework, used to ", "\ncrawl websites and extract structured data from their pages."]}
{"url": ["/url?q=https://www.quora.com/Is-there-a-better-crawler-than-Scrapy&sa=U&ved=0ahUKEwiF3Oak5N_OAhUGCywKHcrzDSA4ChAWCBwwAg&usg=AFQjCNHmaxzAkUvypT3oiQ4hQSgxpNumng"], "text": [" is a really good crawler. Ofcourse if you want to use something besides ", "\n", " then it means you want very specific things. These things that you require", "\n\u00a0..."]}
{"url": ["/url?q=https://anaconda.org/scrapinghub/scrapy&sa=U&ved=0ahUKEwiF3Oak5N_OAhUGCywKHcrzDSA4ChAWCCEwAw&usg=AFQjCNHCWh9PE19IKKVErZuDxJzQeHN4Vw"], "text": ["conda install. linux-64; win-32; win-64; linux-32; osx-64. To install this package ", "\nwith conda run: conda install -c scrapinghub ", "=1.1.0\u00a0..."]}
{"url": ["/url?q=http://mherman.org/blog/2012/11/05/scraping-web-pages-with-scrapy&sa=U&ved=0ahUKEwiF3Oak5N_OAhUGCywKHcrzDSA4ChAWCCcwBA&usg=AFQjCNGs-fRyMHw9nH2sMFSjW2OnlW6X_g"], "text": ["5 Nov 2012 ", " This is a simple tutorial on how to write a crawler using ", " to scrape and ", "\nparse Craigslist Nonprofit jobs in San Francisco and store the data\u00a0..."]}
{"url": ["/url?q=http://ru.stackoverflow.com/questions/tagged/scrapy&sa=U&ved=0ahUKEwiF3Oak5N_OAhUGCywKHcrzDSA4ChAWCC0wBQ&usg=AFQjCNETjsKguVqGcdZ5Lr3OZkiV7rGTrg"], "text": ["23 \u043d\u043e\u044f 2015 ", " \u0420\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044e \u043c\u0435\u0442\u043a\u0438 ", " \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442. ... ", " ", "\n\u043e\u0442\u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432\u0430\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0441\u044b \u0432 middleware \u043f\u043e callback. \u0437\u0430\u043f\u0440\u043e\u0441\u044b\u00a0..."]}
{"url": ["/url?q=http://stackoverflow.com/questions/tagged/scrapy&sa=U&ved=0ahUKEwiF3Oak5N_OAhUGCywKHcrzDSA4ChAWCDMwBg&usg=AFQjCNFhzsnnT7tvfA7klVzwk1QGmPFipA"], "text": [" is a fast open-source high-level screen scraping and web crawling ", "\nframework written in Python, used to crawl websites and extract structured data ", "\nfrom\u00a0..."]}
{"url": ["/url?q=http://www.practicalecommerce.com/articles/90936-Monitor-Competitor-Prices-with-Python-and-Scrapy&sa=U&ved=0ahUKEwiF3Oak5N_OAhUGCywKHcrzDSA4ChAWCDkwBw&usg=AFQjCNHR3e2HCil2w9hHRwTAKWNuShqbNA"], "text": ["29 Jul 2015 ", " Finally, ", " is relatively easy for at least three more reasons: (a) It uses ", "\nPython, a very common and easy to write programming language;\u00a0..."]}
{"url": ["/url?q=https://groups.google.com/d/forum/scrapy-users&sa=U&ved=0ahUKEwiF3Oak5N_OAhUGCywKHcrzDSA4ChAWCEAwCA&usg=AFQjCNE0tFDHX9sMcNBpmot7zSNB4plprQ"], "text": ["-users ... 10+ experience profiles with extensive ", " experience ( Job ", "\nopening), San Datta ... ", " shell not opening page correctly, JEBI93, 8/8/16."]}
{"url": ["/url?q=http://scrapy-cluster.readthedocs.io/&sa=U&ved=0ahUKEwiF3Oak5N_OAhUGCywKHcrzDSA4ChAWCEYwCQ&usg=AFQjCNEGwkjW3dZXA5xW24nIa5dGHigJ_w"], "text": ["This documentation provides everything you need to know about the ", " ... ", "\nrun the Kafka Monitor; API: The default Kafka API the comes with ", " Cluster\u00a0..."]}
{"url": ["/url?q=https://twitter.com/scrapyproject&sa=U&ved=0ahUKEwiH34Ol5N_OAhVKKywKHYtQBzk4FBAWCBMwAA&usg=AFQjCNHQ87W70NGvwqNNMAGwXmN9JflC9A"], "text": ["293 tweets \u2022 24 photos/videos \u2022 3113 followers. Check out the latest Tweets from ", "\n", " (@ScrapyProject)"]}
{"url": ["/url?q=http://codeguida.com/post/254/&sa=U&ved=0ahUKEwiH34Ol5N_OAhVKKywKHYtQBzk4FBAWCBkwAQ&usg=AFQjCNH-yP3YgOY0w_RrtbHVEI1IaiXq6w"], "text": ["7 \u043b\u044e\u0442. 2015 ", " \u041e\u0442\u0440\u0438\u043c\u0443\u0454\u043c\u043e \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0438 \u0442\u0430 \u043f\u043e\u0441\u0438\u043b\u0430\u043d\u043d\u044f \u043d\u0430 \u0441\u0442\u0430\u0442\u0442\u0456 \u0437 \u0430\u0440\u0445\u0456\u0432\u0443 \u043a\u043e\u0434\u0435\u0433\u0456\u0434\u0438 \u0437\u0430 \u0434\u043e\u043f\u043e\u043c\u043e\u0433\u043e\u044e ", "\n", "."]}
{"url": ["/url?q=https://www.npmjs.com/package/node-scrapy&sa=U&ved=0ahUKEwiH34Ol5N_OAhVKKywKHYtQBzk4FBAWCCAwAg&usg=AFQjCNGDUIsQaqDEQ3hNM3vM-NzOw_uYMA"], "text": ["var ", " = require('node-", "'). , url = 'https://github.com/strongloop/express'. ", "\n, selector = '.repository-description'. ", ".scrape(url, selector, function(err,\u00a0..."]}
{"url": ["/url?q=https://blog.monkeylearn.com/creating-sentiment-analysis-model-with-scrapy-and-monkeylearn/&sa=U&ved=0ahUKEwiH34Ol5N_OAhVKKywKHYtQBzk4FBAWCCYwAw&usg=AFQjCNF0DKgK867oKNgN4yr_4I0vmQ-wvA"], "text": ["3 May 2016 ", " On this tutorial we will cover how you can use MonkeyLearn and ", " to build ", "\na machine learning model that will help you analyze vast\u00a0..."]}
{"url": ["/url?q=https://kaismh.wordpress.com/2016/04/29/extracting-data-from-websites-using-scrapy/&sa=U&ved=0ahUKEwiH34Ol5N_OAhVKKywKHYtQBzk4FBAWCCwwBA&usg=AFQjCNGJxqKRQ9nxx1jWjoLfxm1PgfO4Qg"], "text": ["29 Apr 2016 ", " ", " framework is developed in Python, which is preinstalled in Ubuntu and ", "\nalmost all Linux distributions. As of ", " 1.05, it requires\u00a0..."]}
{"url": ["/url?q=https://www.upwork.com/hire/scrapy-framework-freelancers/&sa=U&ved=0ahUKEwiH34Ol5N_OAhVKKywKHYtQBzk4FBAWCDIwBQ&usg=AFQjCNHUpPKq-oQ_GfYFIVlAVas29YeWng"], "text": ["Find freelance ", " programmers and developers for hire. Access 11 ", " ", "\nfreelancers and outsource your project."]}
{"url": ["/url?q=http://newcoder.io/scrape/intro/&sa=U&ved=0ahUKEwiH34Ol5N_OAhVKKywKHYtQBzk4FBAWCDgwBg&usg=AFQjCNH2Jq7UcTB8JblsRnk7_QVZ38WggQ"], "text": ["We will build a web scraper using ", " to scroll through LivingSocial, and save ", "\nlocal deals to our Postgres database. We'll schedule our computer to run this\u00a0..."]}
{"url": ["/url?q=http://kirankoduru.github.io/python/running-scrapy-programmatically.html&sa=U&ved=0ahUKEwiH34Ol5N_OAhVKKywKHYtQBzk4FBAWCD4wBw&usg=AFQjCNGWG5WutfkOYcQjMkRyCe2lioGmZA"], "text": ["24 Jan 2015 ", " The ", " Spider : It is a python class in the ", " framework that is ", "\nresponsible for fetching URLs and parsing the information in the page\u00a0..."]}
{"url": ["/url?q=https://www.nextbigsound.com/labs/scrappy-rabbitmq&sa=U&ved=0ahUKEwiH34Ol5N_OAhVKKywKHYtQBzk4FBAWCEUwCA&usg=AFQjCNGgBj38OlUPWPtHsTxHQ_1SyvmAxw"], "text": ["-rabbitmq is a tool that lets you feed and queue URLs from RabbitMQ via ", "\n", " spiders, using the ", " framework. At Next Big Sound, we've\u00a0..."]}
{"url": ["/url?q=https://news.ycombinator.com/item%3Fid%3D10937129&sa=U&ved=0ahUKEwiH34Ol5N_OAhVKKywKHYtQBzk4FBAWCEowCQ&usg=AFQjCNE2ZWvAXxwXBby7Cwv7FG4o_ogyVA"], "text": ["21 Jan 2016 ", " Sure there are always some edge cases but best way to handle them is to have ", "\nproper validation logic in ", " pipelines - if something is\u00a0..."]}
